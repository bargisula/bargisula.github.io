{
  "tag": "AI專案紀錄",
  "date": "2026-02-27",
  "title": "讓 AI 自己學會買賣股票是什麼感覺？用強化學習做投資決策",
  "desc": "不是讓 AI 預測股價，而是讓它像玩遊戲一樣，從幾千次歷史交易中學出一套「面對不同市場環境該怎麼配置」的策略。",
  "content": "## 這不是在預測股價\n\n很多人一聽到「AI 投資」，第一個念頭是：它能預測明天漲還是跌嗎？\n\n這個專案做的不是這件事。\n\n**它做的是：在不同的市場環境下，決定資金應該怎麼分配。**\n\n進攻還是防守？集中壓一檔個股還是分散在 ETF？這不是預測問題，是決策問題。而解決決策問題，用的是強化學習（Reinforcement Learning）。\n\n---\n\n## 強化學習是什麼？先講一個比喻\n\n你有沒有看過狗狗訓練的影片？每次狗狗做對動作，給一塊零食；做錯了，什麼都沒有，或者輕拍一下鼻子。反覆幾百次，狗學會了「做這個動作有獎勵」。\n\n強化學習就是這個邏輯，只不過學習的是 AI，環境是股票市場，獎勵是投資報酬率。\n\n具體來說：\n\n- **AI（Agent）**：做決定的主角\n- **環境（Environment）**：股票市場，每週給 AI 看當前的市場狀態\n- **動作（Action）**：AI 選擇的策略，共四種\n- **獎勵（Reward）**：一個月的投資報酬率，正的表示賺到了\n\nAI 的目標只有一個：最大化長期獎勵的累計值。\n\n---\n\n## 四種策略，讓 AI 自己挑\n\n這個系統把投資策略簡化成四個選項，每週一次做決定：\n\n**策略 0：防守**\n持有 50% TLT（長期公債 ETF）+ 50% 現金。市場動盪時，債券通常逆向升值，這是避風港配置。\n\n**策略 1：平衡**\nSPY 40%（大盤）+ QQQ 30%（科技）+ 現金 30%。跟著大盤走，不押注也不逃跑。\n\n**策略 2：進攻**\n從 10 個標的中選出近期動量最強的前三名，各 30%，剩下 10% 現金。強者恆強的動量邏輯。\n\n**策略 3：集中**\n把 40%–70% 的資金全押在動量最強的單一個股上。比例不是固定的——個股動量越強，下注越重，最高 70%，最低也有 40% 的保護底。\n\n---\n\n## AI 看的是什麼？44 個數字說明一切\n\n每次 AI 做決定前，它會收到一個包含 44 個數字的「市場快照」，這叫做狀態向量（State Vector）。\n\n**多期動量（30 個數字）**\n\n10 個標的（SPY、QQQ、VTI、TLT、AAPL、MSFT、NVDA、AMZN、META、TSLA），分別計算過去 5 天、20 天、60 天的報酬率。這給了 AI 短中長三個時間維度的動量資訊。\n\n**宏觀環境（4 個數字）**\n\n- **VIX 收盤值**：俗稱「恐慌指數」，數字越高代表市場越害怕\n- **VIX 趨勢**：VIX 相對於自己 20 天均值的偏離程度，反映恐慌是在升溫還是降溫\n- **SPY 均線方向**：大盤是在上升、震盪還是下降趨勢\n- **殖利率利差**：10 年期公債與 3 個月國庫券的利率差。這個數字變負（倒掛）歷史上常是衰退前兆\n\n**持倉狀態（2 個數字）**\n\n當前回撤幅度（從高點跌了多少）和目前採用的策略編號。\n\n**歷史記憶（8 個數字）**\n\n過去 4 週各選了什麼策略、各週報酬率分別是多少。這讓 AI 有「短期記憶」，不會在同一個月內反覆亂換策略。\n\n---\n\n## DQN：讓 AI 學會評估「這個選擇值多少分」\n\n這個專案的核心算法叫做 DQN（Deep Q-Network），2015 年由 DeepMind 提出，原本是拿來讓 AI 打電玩的——那個 AI 自己學會玩 Atari 遊戲，最後打贏人類的故事。\n\n它的核心概念是 **Q 值**：在「這個市場狀態」下，選「這個策略」，預期能拿到多少長期獎勵？\n\n用數學表示就是：Q(狀態, 動作) = 預期未來報酬的折扣總和\n\nAI 用一個神經網路來估算這個 Q 值。訓練的過程就是不斷地問：「我以為這樣做能拿 X 分，結果拿了 Y 分，差了多少？」然後調整網路的權重，讓下次預測更準。這叫做 TD 誤差（Temporal Difference Error），是強化學習的核心更新機制。\n\n訓練資料是 2018 年到今天的所有歷史交易日，大約兩千多個樣本點。每個月四週、每週做一次決策，AI 在歷史上跑了幾萬次模擬，才學出這套策略。\n\n---\n\n## 一些設計決定的理由\n\n**為什麼用動量而不是基本面？**\n\n基本面（本益比、營收成長）更新慢，且難以標準化成統一格式。動量是純數字，任何標的都能算，適合這種多標的的強化學習框架。\n\n**為什麼加殖利率利差？**\n\n殖利率曲線倒掛（短期利率高於長期）在過去 50 年幾乎每次衰退前都出現過，是市場定價中最有資訊量的宏觀訊號之一。讓 AI 能看到這個，相當於給它加了一個宏觀雷達。\n\n**為什麼獎勵只用月報酬率，不做風險調整？**\n\n刻意的選擇。加入夏普比率、最大回撤等複合指標，確實能讓報告數字更好看，但也會讓 AI 的目標函數變複雜，容易過度優化到指標本身而非真實報酬。這個設計認為，簡單的獎勵函數搭配斷頭機制（回撤超過 30% 直接懲罰 -10 分）就夠了。\n\n---\n\n## 這個系統能做什麼，不能做什麼\n\n**能做的：**\n- 每週提供一個基於歷史統計的配置建議\n- 顯示當前宏觀環境（VIX、殖利率曲線、大盤趨勢）\n- 自動計算歷史上類似情境的勝率與平均報酬\n\n**不能做的：**\n- 預測單一股票的漲跌\n- 處理流動性、稅務、最小下單單位等現實限制\n- 保證獲利（過去績效不代表未來結果）\n\n這是一個學習工具，也是一個思考框架：**面對市場，與其猜漲跌，不如問自己現在的環境適合什麼風格的部位。**\n\n---\n\n## 技術規格\n\n| 項目 | 說明 |\n|---|---|\n| **算法** | DQN（Deep Q-Network）with Experience Replay |\n| **框架** | PyTorch + Gymnasium |\n| **標的** | ETF 4 檔（SPY, QQQ, VTI, TLT）+ 個股 6 檔（AAPL, MSFT, NVDA, AMZN, META, TSLA）|\n| **決策頻率** | 每週一次 |\n| **State 維度** | 44 維 |\n| **訓練資料** | 2018 年至今日歷史數據 |\n| **風控機制** | 單月回撤超過 30% 觸發強制出場 |\n| **交易成本** | 買賣各 0.1% |\n\n---\n\n[GitHub 原始碼](https://github.com/bargisula/investment-rl)"
}